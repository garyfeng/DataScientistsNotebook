{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python and SQL with Microsoft SQL Server\n",
    "\n",
    "In this notebook we illustrate how to connect to MS SQL Server via Python in order to convert SQL SELECT queires into pandas data frames.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This demo is based on https://github.com/garyfeng/DataScientistsNotebook. We use `docker-compose` to create a docker cluster with \n",
    "- mssql: Microsoft SQL Server v2017 running on linux\n",
    "- jupyter data science notebook server, as a docker\n",
    "\n",
    "To install:\n",
    "- Make sure you have `Docker` and `git` installed on your computer\n",
    "- In a terminal, `cd` to the folder where you wish to have this project setup, do `git clone https://github.com/garyfeng/DataScientistsNotebook.git`\n",
    "- Go to the downloaded folder, and edit the `.env` file to change the directories to your setup.\n",
    "- Go back to the terminal, do `docker-compose build` and make sure it succeeds\n",
    "\n",
    "To test:\n",
    "- then do `docker-compose up` and make sure all the logs are ok, no errors.\n",
    "- open your browser to hppts://localhost:8888 and login using the Jupyter password set up in the `.env` file\n",
    "- you should see the `work` folder. Click in, and open new notebooks, etc. Note that the MS SQL connection may fail, because you need the IP address of the SQL Server (that is not \"localhost\"). See below.\n",
    "- go back to the terminal, do `docker-compose down` to shut down things\n",
    "\n",
    "To run:\n",
    "- in the terminal, do `docker-compose up -d` to avoid the verbose logs\n",
    "- launch the browser the same way you did in test\n",
    "- you need to find out the IP address for MS SQL Server. In terminal, type `ipconfig` for Windows users or `ifconfig` for mac and linux machines. You will have to look for something like `192.168.56.1` in the printout. On Windows this is typically associated with `VirtualBox`; on macs or linux machines this is typically associated with some words about \"virtual\" but not easy to find. It doesn't hurt to try them all - one of them is for sure to work.\n",
    "- copy that IP address, paste it to the `server` address below in the notebook cell (until I find an automatic method).\n",
    "- now run the `pymssql` code to try to connect, see whether it gives error. Repeate with all IP address until you find one that works ;-)\n",
    "- you can now run the SQL exercises. Your Jupyter notebooks will be saved in your `python/notebooks` folder. \n",
    "- shut down using `docker-compose down` in the terminal; make sure you saved the notebooks first. Your saved notebooks will remain there next time you start the docker cluster, though you need to re-run them as the python environment has been cleared. \n",
    "\n",
    "## Set up \n",
    "\n",
    "Once you are in the Jupyter environment, you may need to install `pymssql` and `elasticsearch` libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymssql in /opt/conda/lib/python3.7/site-packages (2.1.4)\n",
      "Requirement already satisfied: elasticsearch in /opt/conda/lib/python3.7/site-packages (7.6.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from elasticsearch) (1.25.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymssql elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os import getenv\n",
    "\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyMSSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to use for MS SQL Server connection\n",
    "server = getenv(\"MSSQL_SERVERIP\")\n",
    "user = getenv(\"MSSQL_USER\")\n",
    "password = getenv(\"MSSQL_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymssql.Connection at 0x7f205f04c140>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymssql.connect(server, user, password, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Alchemy\n",
    "\n",
    "Which defaults to PyMSSQL anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('mssql+pymssql://{}:{}@{}:1433'.format(user, password, server))\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some data in the SQL Server\n",
    "\n",
    "We now use `pymssql` to create a database `tempdb` and a data table `persons` therein. Will also put in some sample data to play with. \n",
    "\n",
    "We connect, do the above using SQL commands, and then close the connection. We also do a `Select` SQL query there and illustrate how to iterate the results row by row. But going forward we will use `pandas` to convert data into a `dataframe` directly, without having to deal with them one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID=1, Name=John Smith\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# server = getenv(\"PYMSSQL_TEST_SERVER\")\n",
    "# user = getenv(\"PYMSSQL_TEST_USERNAME\")\n",
    "# password = getenv(\"PYMSSQL_TEST_PASSWORD\")\n",
    "\n",
    "conn = pymssql.connect(server, user, password, \"tempdb\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "IF OBJECT_ID('persons', 'U') IS NOT NULL\n",
    "    DROP TABLE persons\n",
    "CREATE TABLE persons (\n",
    "    id INT NOT NULL,\n",
    "    name VARCHAR(100),\n",
    "    salesrep VARCHAR(100),\n",
    "    PRIMARY KEY(id)\n",
    ")\n",
    "\"\"\")\n",
    "cursor.executemany(\n",
    "    \"INSERT INTO persons VALUES (%d, %s, %s)\",\n",
    "    [(1, 'John Smith', 'John Doe'),\n",
    "     (2, 'Jane Doe', 'Joe Dog'),\n",
    "     (3, 'Mike T.', 'Sarah H.')])\n",
    "# you must call commit() to persist your data if you don't set autocommit to True\n",
    "conn.commit()\n",
    "\n",
    "cursor.execute('SELECT * FROM persons WHERE salesrep=%s', 'John Doe')\n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "    print(\"ID=%d, Name=%s\" % (row[0], row[1]))\n",
    "    row = cursor.fetchone()\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL query using Pandas read_sql_query\n",
    "\n",
    "Pandas supports the function [read_sql_query](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html) to execute a SQL `Select` query and convert the data into a data frame. See tutorial at https://datatofish.com/sql-to-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>salesrep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>Joe Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mike T.</td>\n",
       "      <td>Sarah H.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        name  salesrep\n",
       "0   1  John Smith  John Doe\n",
       "1   2    Jane Doe   Joe Dog\n",
       "2   3     Mike T.  Sarah H."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = pymssql.connect(server, user, password, \"tempdb\")\n",
    "\n",
    "df = pd.read_sql_query(\n",
    "    '''SELECT * FROM persons''', conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the data to Elasticsearch\n",
    "\n",
    "We send this data frame to the `elasticsearch` server instance that we are running as part of the docker-compose setup. The ES server address on the same IP as `mssql` server, just different ports. \n",
    "\n",
    "Taking some examples from https://towardsdatascience.com/exporting-pandas-data-to-elasticsearch-724aa4dd8f62\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a connection (client) to the ES server\n",
    "es_client = Elasticsearch([\n",
    "        'http://{}:9200/'.format(server)\n",
    "    ], http_compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to get data into a shape we can post to ES, with `index='datasciencenotebook_index'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_index='datasciencenotebook_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator(df):\n",
    "    df_iter = df.iterrows()\n",
    "    for index, document in df_iter:\n",
    "        yield {\n",
    "                \"_index\": es_index,\n",
    "                \"_type\": \"_doc\",\n",
    "                \"_id\" : f\"{document['id']}\",\n",
    "                \"_source\": document.to_dict(),\n",
    "            }\n",
    "    #raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now post the data to elastic search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.bulk(es_client,doc_generator(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search ES for the index. We get a `json` object, with the result under `['hits']['hits']`. We then take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es_client.search(index=es_index, body={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'datasciencenotebook_index',\n",
       "  '_type': '_doc',\n",
       "  '_id': '1',\n",
       "  '_score': 1.0,\n",
       "  '_source': {'id': 1, 'name': 'John Smith', 'salesrep': 'John Doe'}},\n",
       " {'_index': 'datasciencenotebook_index',\n",
       "  '_type': '_doc',\n",
       "  '_id': '2',\n",
       "  '_score': 1.0,\n",
       "  '_source': {'id': 2, 'name': 'Jane Doe', 'salesrep': 'Joe Dog'}},\n",
       " {'_index': 'datasciencenotebook_index',\n",
       "  '_type': '_doc',\n",
       "  '_id': '3',\n",
       "  '_score': 1.0,\n",
       "  '_source': {'id': 3, 'name': 'Mike T.', 'salesrep': 'Sarah H.'}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using list comprehension we get the data into a shape that we can read in to pandas as a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>salesrep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>John Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>Joe Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mike T.</td>\n",
       "      <td>Sarah H.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        name  salesrep\n",
       "0   1  John Smith  John Doe\n",
       "1   2    Jane Doe   Joe Dog\n",
       "2   3     Mike T.  Sarah H."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([row['_source'] for row in res[\"hits\"][\"hits\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index on Kibana and visualize\n",
    "\n",
    "Also on the same `docker-compose` network is an instance of Kibana, at http://localhost:5601/. Launch a browser at http://localhost:5601/, and you should see the Kibana user interface. \n",
    "\n",
    "Before you can visualize, you need to create an index of the data we just sent. In Kibana web interface, find the *gear icon* (management) on the buttom-left handside. You will see \"Kibana\" then click `Index Patterns`. Click that link, then click `\"Create Index Pattern\"` button. Type the index name for the data we just put in Elastersearch. As you type, Kibana will find the index. Click `\"Next\"` when you find a match. Click `\"Create Index Pattern\"` to complete.\n",
    "\n",
    "Once that's done, click the `discover` icon at the top right to see the data in Elasticsearch. Confirm that's the data you just posted. \n",
    "\n",
    "Then you can use Kibana to visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
